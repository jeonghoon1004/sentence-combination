# from ckonlpy.tag import Twitter # ì•„ë˜ Oktë¡œ ì „í™˜ë¨
import json
import re

from konlpy.tag import Okt
from konlpy.utils import pprint
from konlpy.tag import Komoran

# from krwordrank.word import KRWordRank, summarize_with_keywords
# from krwordrank.sentence import summarize_with_sentences

# from extract import extractExcel

# from wordcloud import WordCloud
# import matplotlib.pyplot as plt
# from hanspell import spell_checker

import torch
# from fastai.text.all import *
# import fastai
# from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast

# from torch.utils.data import DataLoader, Dataset
# from transformers.optimization import AdamW, get_cosine_schedule_with_warmup

class koreanTokenizer:
    def __init__ (self):
        self.okt = Okt()
    def addNoun(self):
        self.okt.nouns('ë‹¨ì–´') # ë‹¨ì–´ì‚¬ì „ì— ì¶”ê°€
    def morph(self, text):
        return self.okt.morphs(text)
        # Twitter.add_dictionary('ë‹¨ì–´', 'Noun') # ë‹¨ì–´ì‚¬ì „ì— ì¶”ê°€ *êµ¬ë²„ì „
    def pos(self, text):
        return self.okt.pos(text)
    def noun(self, text):
        return self.okt.nouns(text)
    def prase(self, text):
        return self.okt.phrases(text)
    def getTags(self, text, filter = None ):
        self.okt.normalize
        prep = self.okt.pos(text)
        matched = []
        for dict in prep:
            name = dict[0] # ì›ë¬¸
            if name in filter:
                matched.append(name)
        return "matched tag {}".format(matched) if len(matched) > 0 else None
class kmran:
    def __init__(self) -> None:
        self.kmr = Komoran(userdic="data/dict.txt")
        
        pass
    def morph(self, text):
        try:
            return self.kmr.morphs(text)
        except:
            print("error text : {}".format(text))
    def pos(self, text):
        try:
            return self.kmr.pos(text, flatten=True)
        except:
            print("error text : {}".format(text))
    def nouns(self, text):
        try:
            # spelled_sent = spell_checker.check(text)
            # hanspell_sent = spelled_sent.checked
            # return self.kmr.nouns(hanspell_sent)
            return self.kmr.nouns(text)
        except:
            print("error text : {}".format(text))
            # return self.kmr.nouns(text)
    def getTags(self, text, filter = None ):
        try:
            prep = self.kmr.pos(text, flatten=False)
            matched = []
            for dict in prep:
                name = dict[0]
                if name in filter:
                    matched.append(name)
            return "matched tag {}".format(matched) if len(matched) > 0 else None
        except:
            print("error text : {}".format(text))
    def unkownChecker(self):
        path_temp_json = 'data/test_sentence_json.json'
        with open(path_temp_json, "r", encoding="utf-8") as f:
                js_obj = json.load(f)
                OptionList = []
                for key in js_obj:
                    OptionList.append(key)
        for text in js_obj["ëª¨í˜¸í•¨"]:
            pprint( kmran().pos(text) )
# class rank:
#     def __init__(self) -> None:
#         pass
#     def extractBest(self, stopwords=None ):
#         if stopwords is None:
#             stopwords = {'ë¦¼í”„', 'ë§ˆì‚¬ì§€', 'ì„¸íƒê¸°', 'ê³°íŒ¡ì´', 'í›„ë“œ'}
#         penalty = lambda x:0 if (25 <= len(x) <= 80) else 1
#         keywords, sents = summarize_with_sentences( texts, penalty=penalty, stopwords=stopwords, diversity=0.5, num_keywords=100, num_keysents=10, verbose=False )
#         print('sentence : %s' % (sents))
        
#     def drawKewords(self):
#         passwords = {
#             word:score for word, score in sorted(keywords.items(), key=lambda x:-x[1])[:300] if not (word in stopwords)
#         }
#         krwordrank_cloud = WordCloud(
#             font_path = font_path,
#             width = 800,
#             height = 800,
#             background_color="white"
#         )
#         krwordrank_cloud = krwordrank_cloud.generate_from_frequencies(passwords)
#         fig = plt.figure(figsize=(10, 10))
#         plt.imshow(krwordrank_cloud, interpolation="bilinear")
#         plt.show()
        # fig.savefig('./wordcloud.png')
    
# TODO keras ê¸°ë°˜ seq2seq LSTM ëª¨ë¸, KoGPT-2 ëª¨ë¸ ë”¥ëŸ¬ë‹ 
# LSTM 25~100ì ê¸´ë¬¸ì¥ ê³¼ì í•©ë°œìƒì´ìŠˆ  
# - epoch 50 batch size 64 50ë²ˆ ë°˜ë³µ; 12ë§Œ ë¦¬ë·° ê²€ì¦ì‹œ 70% ëª©í‘œì¹˜  ë°ì´í„° ì–‘ ë¶€ì¡±í•˜ë©´ ì¼ë°˜ì  ë°ì´í„° íŒ¨í„´ì„ í•™ìŠµ ë¬¸ì œ
# bahdanau attention ì ìš© seq2seqëª¨ë¸
# - epoch 100 batch size 2 ê²€ì¦ì •í™•ë„ëŠ” ì˜¤ë¦„ - í›ˆë ¨ ì •í™•ë„ê°€ ë‚®ì•„ì„œ ê·¸ëŸ°ì§€ ë‹¤ë¥¸ ë¦¬ë·°ìƒì„±ì´ìŠˆ
# [ ] KoGPT í•œê¸€ ì‚¬ì „í•™ìŠµ  Transformer decoder 12ì¸µ, BPE ê¸°ë°˜ SentencePiece í† í°í™” ë°©ì‹
# - ìƒê¸° 2 ëª¨ë¸ë³´ë‹¤ëŠ” ì •í™•í•˜ì§€ë§Œ ì–´ìƒ‰í•œ ë¬¸ì¥ì´ ì—°ê²°ë¨ í›ˆë ¨90%ì´í•˜ ê²€ì¦ 93% ë°ì´í„° 12ë§Œê°œ ë¦¬ë·°
# class ReviewGenerator:
#     def __init__(self) -> None:
#         path_temp_json = 'data/test_sentence_json.json'
        
#         torch.manual_seed(42)
#         # ê¸°ë³¸ ë² ì´ìŠ¤ëŠ” ì‹ ë¬¸ê¸°ì‚¬, ì²­ì™€ëŒ€ ì—°ì„¤ë¬¸ ë“±ìœ¼ë¡œ ë¦¬ë·°ê´€ë ¨ëœ ì¶”ê°€í•™ìŠµì´ í•„ìš”
#         self.tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2", 
#                                                                  bos_token='</s>', 
#                                                                  eos_token='</s>', 
#                                                                  unk_token='<unk>',
#                                                                  pad_token='<pad>', 
#                                                                  mask_token='<mask>')
#         self.model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')
        
#         with open(path_temp_json, "r", encoding="utf-8") as f:
#             data_json = json.load(f)
#         keys = data_json.keys()
#         data = []
#         for k in keys:
#             data.append( data_json[k] )
#         tls = TfmdLists(data, TransformersTokenizer(self.tokenizer))
#         batch,seq_len = 8,256
#         # dls = tls.dataloaders(bs=batch, seq_len=seq_len)
#         # dls.show_batch(max_n=2)
        
#         pass
#     def generator(self, text) -> None:
#         input_ids = self.tokenizer.encode(text, return_tensors='pt')
#         gen_ids = self.model.generate(input_ids, max_length=128, 
#                                       repetition_penalty=2.0, 
#                                       pad_token_id=self.tokenizer.pad_token_id,
#                                       eos_token_id=self.tokenizer.eos_token_id,
#                                       bos_token_id=self.tokenizer.bos_token_id,
#                                       use_cache=True)
#         generated = self.tokenizer.decode(gen_ids[0])
#         print(generated)
#         pass

# class TransformersTokenizer(Transform):
#     def __init__(self, tokenizer) -> None:
#         self.tokenizer = tokenizer
#     def encode(self, x):
#         toks = self.tokenizer.tokenize(x)
#         return torch.tensor(self.tokenizer.convert_tokens_to_ids(toks))
#     def decodes(self, x):
#         return TitledStr(self.tokenizer.decode(x.cpu().numpy()))

if __name__ == "__main__": # ì§ì ‘ ì‹¤í–‰ëœ ëª¨ë“ˆì¼ ê²½ìš°
    print(torch.__version__)
    korText = "ë¹ ë¥¸ ë°°ì†¡ ì¢‹ì€ í’ˆì§ˆ êµ¿ì…ë‹ˆë‹¤." # test
    test_str = 'ì¢‹ì•„ì§€ëŠ” ê²ƒ ê°™ì•„ìš”ğŸ¥°ğŸ¥°'
    filter = ["ë°°ì†¡", "í’ˆì§ˆ", "ì„œë¹„ìŠ¤", "ë§Œì¡±"]
    stopwords = {'ë¦¼í”„', 'ë§ˆì‚¬ì§€', 'ì„¸íƒê¸°', 'ê³°íŒ¡ì´', 'í›„ë“œ'}
    font_path = 'font/NanumGothic.ttf' # wordcloud hangul font
    
    only_BMP_pattern = re.compile("["
        u"\U00010000-\U0010FFFF"  #BMP characters ì´ì™¸
                "]+", flags=re.UNICODE)
    test_str = only_BMP_pattern.sub(r'', test_str)    

    print(test_str)
    # pprint( koreanTokenizer().getTags(korText, filter=filter) )
    # pprint( kmran().getTags(korText, filter=filter) )
    
    pprint( kmran().getTags(test_str, filter=stopwords))
    pprint( kmran().nouns(test_str) )
    pprint( kmran().pos(test_str) )
    
    # path_temp_json = 'data/test_sentence_json.json'
    # with open(path_temp_json, "r", encoding="utf-8") as f:
    #         js_obj = json.load(f)
    #         OptionList = []
    #         for key in js_obj:
    #             OptionList.append(key)
    # for text in js_obj["ëª¨í˜¸í•¨"]:
    #     text = only_BMP_pattern.sub(r'', text)
    #     # pprint( kmran().pos(text) )
    #     pprint( kmran().nouns(text) )
    
    # gen = ReviewGenerator()
    # gen.generator("ë°°ì†¡ ê°ì‚¬")
    
    # quit()
        
#     # test TextJson
#     extracts = extractExcel()
#     extracts.getText()
#     texts = extracts.TextList
#     print("json {} sentence loaded".format(len(texts)))
    
    
#     min_count = 5
#     max_length = 10
#     wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length) #ì „ì²˜ë¦¬
    
#     beta = 0.85
#     max_iter = 10
#     keywords, rank, graph = wordrank_extractor.extract(texts, beta, max_iter) #ì¶”ì¶œ
#     for word, r in sorted(keywords.items(), key=lambda x:x[1], reverse=True)[:30]:
#         print('%8s:\t%.4f' % (word, r))
    
    
#     # passwords = {
#     #     word:score for word, score in sorted(keywords.items(), key=lambda x:-x[1])[:300] if not (word in stopwords)
#     # }
#     # keywords = summarize_with_keywords(texts) # with default arguments
#     # keywords = summarize_with_keywords(texts, min_count=5, max_length=10,
#     #     beta=0.85, max_iter=10, stopwords=stopwords, verbose=True)    
#     # for word, r in sorted(keywords.items(), key=lambda x:x[1], reverse=True)[:30]:
#     #     print('%8s:\t%.4f' % (word, r))
    
#     # default sentence
#     # keywords, sents = summarize_with_sentences(texts, num_keywords=100, num_keysents=10)
#     # print('sentence : %s' % (sents)) # num_keysents ê°œ ìµœëŒ€ì¡°í•© ë¦¬ë·°ê¸€
    
#     # ê¸¸ì´ì œí•œ, ê¸ˆì§€ì–´ í¬í•¨
#     penalty = lambda x:0 if (25 <= len(x) <= 80) else 1
#     keywords, sents = summarize_with_sentences( texts, penalty=penalty, stopwords=stopwords, diversity=0.5, num_keywords=10, num_keysents=10, verbose=False)
#     print('dict : %s' % (keywords))
#     print('sentence 2 : %s' % (sents))
    


# # ========================================================================
# def practice():
#     korText = "ë¹ ë¥¸ ë°°ì†¡ ì¢‹ì€ í’ˆì§ˆ êµ¿ì…ë‹ˆë‹¤." # test
#     filter = ["ë°°ì†¡", "í’ˆì§ˆ", "ì„œë¹„ìŠ¤", "ë§Œì¡±"]
    
#     font_path = 'font/NanumGothic.ttf' # wordcloud hangul font
    
#     # pprint( koreanTokenizer().getTags(korText, filter=filter) )
#     # pprint( kmran().getTags(korText, filter=filter) )
    
#     extracts = extractExcel()
#     extracts.getText()
#     texts = extracts.TextList
#     print(len(texts))
    
#     min_count = 5
#     max_length = 10
#     wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)
    
#     beta = 0.85
#     max_iter = 10
#     keywords, rank, graph = wordrank_extractor.extract(texts, beta, max_iter)
#     for word, r in sorted(keywords.items(), key=lambda x:x[1], reverse=True)[:30]:
#         print('%8s:\t%.4f' % (word, r))
    
#     stopwords = {'ë¦¼í”„', 'ë§ˆì‚¬ì§€', 'ì„¸íƒê¸°', 'ê³°íŒ¡ì´'}
#     passwords = {
#         word:score for word, score in sorted(keywords.items(), key=lambda x:-x[1])[:300] if not (word in stopwords)
#     }
#     # keywords = summarize_with_keywords(texts) # with default arguments
#     keywords = summarize_with_keywords(texts, min_count=5, max_length=10,
#         beta=0.85, max_iter=10, stopwords=stopwords, verbose=True)    
#     for word, r in sorted(keywords.items(), key=lambda x:x[1], reverse=True)[:30]:
#         print('%8s:\t%.4f' % (word, r))
    
#     # Set your font path
    
#     # krwordrank_cloud = WordCloud(
#     #     font_path = font_path,
#     #     width = 800,
#     #     height = 800,
#     #     background_color="white"
#     # )
#     # krwordrank_cloud = krwordrank_cloud.generate_from_frequencies(passwords)
    
#     # fig = plt.figure(figsize=(10, 10))
#     # plt.imshow(krwordrank_cloud, interpolation="bilinear")
#     # plt.show()
#     # fig.savefig('./wordcloud.png')
    
#     # default sentence
#     keywords, sents = summarize_with_sentences(texts, num_keywords=100, num_keysents=10)
#     print('sentence : %s' % (sents)) # num_keysents ê°œ ìµœëŒ€ì¡°í•© ë¦¬ë·°ê¸€
    
#     # ê¸¸ì´ì œí•œ, ê¸ˆì§€ì–´ í¬í•¨
#     penalty = lambda x:0 if (25 <= len(x) <= 80) else 1
#     keywords, sents = summarize_with_sentences( texts, penalty=penalty, stopwords=stopwords, diversity=0.5, num_keywords=100, num_keysents=10, verbose=False)
#     print('sentence 2 : %s' % (sents))